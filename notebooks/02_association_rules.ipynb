{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebdf08db",
   "metadata": {},
   "source": [
    "# Notebook 02 — Apriori Association Rules\n",
    "\n",
    "**Purpose:** Find which products are frequently bought together using the Apriori\n",
    "algorithm. Produces `co_purchase_rules.json` — a lightweight lookup used by\n",
    "the recommendation engine at runtime.\n",
    "\n",
    "**Key metrics:**\n",
    "- **Support:** how often the pair appears across all orders\n",
    "- **Confidence:** P(B | A) — given A is bought, how likely is B?\n",
    "- **Lift:** how much more likely B is when A is present vs random\n",
    "\n",
    "**Input:** `order_baskets.pkl`, `item_catalog.json`\n",
    "\n",
    "**Output:** `co_purchase_rules.json`\n",
    "\n",
    "**Runtime:** ~5–10 min on Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c95c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass  # warnings cell removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e23a5e6-8160-4bbd-b2b6-20538d99bc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ranaraunitrazsingh/Desktop/Placements Assignments/Unthinkable solutions/voice-shopping-assistant/instacart-env/bin/python'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df6cc405-bc15-44db-953f-c6f9de924dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-23.0.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.1 kB)\n",
      "Collecting mlxtend\n",
      "  Using cached mlxtend-0.24.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Using cached scipy-1.17.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is looking at multiple versions of mlxtend to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mlxtend\n",
      "  Using cached mlxtend-0.23.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pandas>=0.24.2 (from mlxtend)\n",
      "  Using cached pandas-3.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (79 kB)\n",
      "Collecting matplotlib>=3.0.0 (from mlxtend)\n",
      "  Using cached matplotlib-3.10.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.0.0->mlxtend)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.0.0->mlxtend)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.0.0->mlxtend)\n",
      "  Using cached fonttools-4.61.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.0.0->mlxtend)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting packaging>=20.0 (from matplotlib>=3.0.0->mlxtend)\n",
      "  Using cached packaging-26.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib>=3.0.0->mlxtend)\n",
      "  Using cached pillow-12.1.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib>=3.0.0->mlxtend)\n",
      "  Using cached pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting python-dateutil>=2.7 (from matplotlib>=3.0.0->mlxtend)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl (8.1 MB)\n",
      "Using cached pyarrow-23.0.1-cp312-cp312-macosx_12_0_arm64.whl (34.2 MB)\n",
      "Using cached mlxtend-0.23.4-py3-none-any.whl (1.4 MB)\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached matplotlib-3.10.8-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl (273 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.61.1-cp312-cp312-macosx_10_13_universal2.whl (2.9 MB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached packaging-26.0-py3-none-any.whl (74 kB)\n",
      "Using cached pandas-3.0.1-cp312-cp312-macosx_11_0_arm64.whl (9.9 MB)\n",
      "Using cached pillow-12.1.1-cp312-cp312-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached scipy-1.17.1-cp312-cp312-macosx_14_0_arm64.whl (20.3 MB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, six, pyparsing, pyarrow, pillow, packaging, numpy, kiwisolver, joblib, fonttools, cycler, scipy, python-dateutil, contourpy, scikit-learn, pandas, matplotlib, mlxtend\n",
      "\u001b[2K  Attempting uninstall: threadpoolctl\n",
      "\u001b[2K    Found existing installation: threadpoolctl 3.6.0\n",
      "\u001b[2K    Uninstalling threadpoolctl-3.6.0:\n",
      "\u001b[2K      Successfully uninstalled threadpoolctl-3.6.00m \u001b[32m 0/18\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: six━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/18\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Found existing installation: six 1.17.0━\u001b[0m \u001b[32m 0/18\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Uninstalling six-1.17.0:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/18\u001b[0m [threadpoolctl]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0━━━\u001b[0m \u001b[32m 0/18\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: pyparsing━━━━━━━━━━━\u001b[0m \u001b[32m 0/18\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Found existing installation: pyparsing 3.3.2 \u001b[32m 0/18\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Uninstalling pyparsing-3.3.2:━━━━━━━━━━━\u001b[0m \u001b[32m 0/18\u001b[0m [threadpoolctl]\n",
      "\u001b[2K      Successfully uninstalled pyparsing-3.3.20m \u001b[32m 0/18\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: pyarrow━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/18\u001b[0m [pyparsing]\n",
      "\u001b[2K    Found existing installation: pyarrow 23.0.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/18\u001b[0m [pyparsing]\n",
      "\u001b[2K    Uninstalling pyarrow-23.0.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/18\u001b[0m [pyparsing]\n",
      "\u001b[2K      Successfully uninstalled pyarrow-23.0.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/18\u001b[0m [pyparsing]\n",
      "\u001b[2K  Attempting uninstall: pillow90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: pillow 12.1.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling pillow-12.1.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled pillow-12.1.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/18\u001b[0m [pyarrow]\n",
      "\u001b[2K  Attempting uninstall: packaging0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: packaging 26.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling packaging-26.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled packaging-26.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: numpy0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/18\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: kiwisolverm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/18\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: kiwisolver 1.4.9━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [kiwisolver]\n",
      "\u001b[2K    Uninstalling kiwisolver-1.4.9:[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [kiwisolver]\n",
      "\u001b[2K      Successfully uninstalled kiwisolver-1.4.9━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [kiwisolver]\n",
      "\u001b[2K  Attempting uninstall: joblib[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [kiwisolver]\n",
      "\u001b[2K    Found existing installation: joblib 1.5.3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [kiwisolver]\n",
      "\u001b[2K    Uninstalling joblib-1.5.3:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [kiwisolver]\n",
      "\u001b[2K      Successfully uninstalled joblib-1.5.3━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/18\u001b[0m [kiwisolver]\n",
      "\u001b[2K  Attempting uninstall: fonttoolsm╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [joblib]\n",
      "\u001b[2K    Found existing installation: fonttools 4.61.1━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [joblib]\n",
      "\u001b[2K    Uninstalling fonttools-4.61.1:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [joblib]\n",
      "\u001b[2K      Successfully uninstalled fonttools-4.61.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/18\u001b[0m [joblib]\n",
      "\u001b[2K  Attempting uninstall: cycler0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/18\u001b[0m [fonttools]\n",
      "\u001b[2K    Found existing installation: cycler 0.12.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/18\u001b[0m [fonttools]\n",
      "\u001b[2K    Uninstalling cycler-0.12.1:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/18\u001b[0m [fonttools]\n",
      "\u001b[2K      Successfully uninstalled cycler-0.12.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/18\u001b[0m [fonttools]\n",
      "\u001b[2K  Attempting uninstall: scipy━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [cycler]\n",
      "\u001b[2K    Found existing installation: scipy 1.17.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [cycler]\n",
      "\u001b[2K    Uninstalling scipy-1.17.1:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [cycler]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.17.10m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/18\u001b[0m [cycler]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0━━\u001b[0m \u001b[32m11/18\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:0m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: contourpy[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: contourpy 1.3.3━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling contourpy-1.3.3:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled contourpy-1.3.3m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: scikit-learnm╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: scikit-learn 1.8.0━━━━━━━━━━━\u001b[0m \u001b[32m11/18\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling scikit-learn-1.8.0:━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m14/18\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled scikit-learn-1.8.0\u001b[90m━━━━━━━━\u001b[0m \u001b[32m14/18\u001b[0m [scikit-learn]\n",
      "\u001b[2K  Attempting uninstall: pandas━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m14/18\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Found existing installation: pandas 3.0.1\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m14/18\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Uninstalling pandas-3.0.1:━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m14/18\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled pandas-3.0.1m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m14/18\u001b[0m [scikit-learn]\n",
      "\u001b[2K  Attempting uninstall: matplotlib━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m15/18\u001b[0m [pandas]n]\n",
      "\u001b[2K    Found existing installation: matplotlib 3.10.8m\u001b[90m━━━━━━\u001b[0m \u001b[32m15/18\u001b[0m [pandas]\n",
      "\u001b[2K    Uninstalling matplotlib-3.10.8:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m15/18\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled matplotlib-3.10.8[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m15/18\u001b[0m [pandas]\n",
      "\u001b[2K  Attempting uninstall: mlxtend━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m16/18\u001b[0m [matplotlib]\n",
      "\u001b[2K    Found existing installation: mlxtend 0.23.4m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m16/18\u001b[0m [matplotlib]\n",
      "\u001b[2K    Uninstalling mlxtend-0.23.4:━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m16/18\u001b[0m [matplotlib]\n",
      "\u001b[2K      Successfully uninstalled mlxtend-0.23.491m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m16/18\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [mlxtend]7/18\u001b[0m [mlxtend]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 joblib-1.5.3 kiwisolver-1.4.9 matplotlib-3.10.8 mlxtend-0.23.4 numpy-1.26.4 packaging-26.0 pandas-3.0.1 pillow-12.1.1 pyarrow-23.0.1 pyparsing-3.3.2 python-dateutil-2.9.0.post0 scikit-learn-1.8.0 scipy-1.17.1 six-1.17.0 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/Users/ranaraunitrazsingh/Desktop/Placements Assignments/Unthinkable solutions/voice-shopping-assistant/instacart-env/bin/python', '-m', 'pip', 'install', '--force-reinstall', 'numpy==1.26.4', 'scikit-learn', 'pyarrow', 'mlxtend'], returncode=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\",\n",
    "     \"--force-reinstall\",\n",
    "     \"numpy==1.26.4\",\n",
    "     \"scikit-learn\",\n",
    "     \"pyarrow\",\n",
    "     \"mlxtend\"],\n",
    "    check=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab047fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlxtend imported OK\n",
      "OUTPUT_DIR = ../data/output\n"
     ]
    }
   ],
   "source": [
    "import os, json, pickle, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "IS_KAGGLE  = os.path.exists(\"/kaggle/input\")\n",
    "OUTPUT_DIR = \"/kaggle/working\" if IS_KAGGLE else \"../data/output\"\n",
    "MODELS_DIR = \"/kaggle/working\" if IS_KAGGLE else \"../data/models\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "print(\"mlxtend imported OK\")\n",
    "print(f\"OUTPUT_DIR = {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49442aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Loading data...\n",
      "============================================================\n",
      "Loaded 2,849,883 baskets\n",
      "Catalog: 3000 items\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Loading data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/order_baskets.pkl\", \"rb\") as f:\n",
    "    baskets = pickle.load(f)\n",
    "with open(f\"{OUTPUT_DIR}/item_catalog.json\", \"r\") as f:\n",
    "    item_catalog = json.load(f)\n",
    "\n",
    "name_to_category = {item[\"name_lower\"]: item[\"category\"] for item in item_catalog}\n",
    "print(f\"Loaded {len(baskets):,} baskets\")\n",
    "print(f\"Catalog: {len(item_catalog)} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae21e736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: Subsampling for memory efficiency...\n",
      "============================================================\n",
      "Sampled 200,000 of 2,849,883 baskets\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Subsampling for memory efficiency...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 200K baskets × 3000 products ≈ 600MB boolean matrix — safe on Kaggle 16GB\n",
    "# Reduce to 100_000 if you hit MemoryError\n",
    "SAMPLE_SIZE = 200_000\n",
    "\n",
    "if len(baskets) > SAMPLE_SIZE:\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.choice(len(baskets), SAMPLE_SIZE, replace=False)\n",
    "    baskets_sample = [baskets[i] for i in idx]\n",
    "    print(f\"Sampled {SAMPLE_SIZE:,} of {len(baskets):,} baskets\")\n",
    "else:\n",
    "    baskets_sample = baskets\n",
    "    print(f\"Using all {len(baskets_sample):,} baskets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f153035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: Encoding baskets (TransactionEncoder)...\n",
      "============================================================\n",
      "Encoded in 4.5s\n",
      "Matrix shape: (200000, 3000)  (baskets × unique products)\n",
      "Memory: 572 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Encoding baskets (TransactionEncoder)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "t0 = time.time()\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(baskets_sample).transform(baskets_sample)\n",
    "basket_df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "print(f\"Encoded in {time.time()-t0:.1f}s\")\n",
    "print(f\"Matrix shape: {basket_df.shape}  (baskets × unique products)\")\n",
    "print(f\"Memory: {basket_df.memory_usage(deep=True).sum() / 1024**2:.0f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fec03d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: Running Apriori (2–5 min)...\n",
      "============================================================\n",
      "Done in 20.0s\n",
      "Frequent itemsets: 4,342\n",
      "  Singletons: 1,989\n",
      "  Pairs:      2,353\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: Running Apriori (2–5 min)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TUNABLE — reduce min_support if you get too few rules; increase if too slow\n",
    "MIN_SUPPORT    = 0.001   # item pair must appear in ≥0.5% of orders\n",
    "MAX_LEN        = 2       # pairwise only (triples are slow and rarely needed)\n",
    "\n",
    "t0 = time.time()\n",
    "frequent_itemsets = apriori(\n",
    "    basket_df,\n",
    "    min_support=MIN_SUPPORT,\n",
    "    use_colnames=True,\n",
    "    max_len=MAX_LEN,\n",
    "    low_memory=True,\n",
    ")\n",
    "print(f\"Done in {time.time()-t0:.1f}s\")\n",
    "print(f\"Frequent itemsets: {len(frequent_itemsets):,}\")\n",
    "print(f\"  Singletons: {len(frequent_itemsets[frequent_itemsets['itemsets'].apply(len)==1]):,}\")\n",
    "print(f\"  Pairs:      {len(frequent_itemsets[frequent_itemsets['itemsets'].apply(len)==2]):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35e26d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: Extracting association rules...\n",
      "============================================================\n",
      "Association rules after filters: 470\n",
      "\n",
      "Top 15 rules by confidence:\n",
      "  Zero Calorie Cola                        → Soda                                      conf=0.50  lift=45.4\n",
      "  Organic Yellow Squash                    → Organic Zucchini                          conf=0.49  lift=13.0\n",
      "  Yotoddler Organic Pear Spinach Mango Yogurt → Organic Whole Milk Strawberry Beet Berry Yogurt Pouch  conf=0.48  lift=205.4\n",
      "  Non Fat Acai & Mixed Berries Yogurt      → Icelandic Style Skyr Blueberry Non-fat Yogurt  conf=0.47  lift=67.5\n",
      "  Total 2% Lowfat Greek Strained Yogurt With Blueberry → Total 2% with Strawberry Lowfat Greek Strained Yogurt  conf=0.47  lift=45.4\n",
      "  Non Fat Raspberry Yogurt                 → Icelandic Style Skyr Blueberry Non-fat Yogurt  conf=0.44  lift=63.6\n",
      "  Organic Whole Milk Strawberry Beet Berry Yogurt Pouch → Yotoddler Organic Pear Spinach Mango Yogurt  conf=0.44  lift=205.4\n",
      "  Nonfat Icelandic Style Strawberry Yogurt → Icelandic Style Skyr Blueberry Non-fat Yogurt  conf=0.43  lift=62.2\n",
      "  Total 2% Lowfat Greek Strained Yogurt with Peach → Total 2% with Strawberry Lowfat Greek Strained Yogurt  conf=0.43  lift=41.5\n",
      "  Bartlett Pears                           → Banana                                    conf=0.40  lift=2.4\n",
      "  Lemon Sparkling Water                    → Grapefruit Sparkling Water                conf=0.39  lift=76.5\n",
      "  Organic Fuji Apple                       → Banana                                    conf=0.39  lift=2.4\n",
      "  Gala Apples                              → Banana                                    conf=0.38  lift=2.3\n",
      "  Vanilla Skyr Nonfat Yogurt               → Icelandic Style Skyr Blueberry Non-fat Yogurt  conf=0.38  lift=54.8\n",
      "  Icelandic Style Skyr Blueberry Non-fat Yogurt → Non Fat Raspberry Yogurt                  conf=0.38  lift=63.6\n",
      "\n",
      "Saved full rules: ../data/models/apriori_rules.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: Extracting association rules...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "MIN_CONFIDENCE = 0.2\n",
    "MIN_LIFT       = 1.0\n",
    "\n",
    "rules = association_rules(\n",
    "    frequent_itemsets,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=MIN_CONFIDENCE,\n",
    "    num_itemsets=len(frequent_itemsets),\n",
    ")\n",
    "rules = rules[rules[\"lift\"] >= MIN_LIFT]\n",
    "print(f\"Association rules after filters: {len(rules):,}\")\n",
    "\n",
    "print(\"\\nTop 15 rules by confidence:\")\n",
    "top = rules.sort_values(\"confidence\", ascending=False).head(15)\n",
    "for _, r in top.iterrows():\n",
    "    a = list(r[\"antecedents\"])[0]\n",
    "    c = list(r[\"consequents\"])[0]\n",
    "    print(f\"  {a:40s} → {c:40s}  conf={r['confidence']:.2f}  lift={r['lift']:.1f}\")\n",
    "\n",
    "rules.to_pickle(f\"{MODELS_DIR}/apriori_rules.pkl\")\n",
    "print(f\"\\nSaved full rules: {MODELS_DIR}/apriori_rules.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c34c18fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 6: Building co-purchase JSON lookup...\n",
      "============================================================\n",
      "Saved: ../data/output/co_purchase_rules.json\n",
      "Products with rules:    336\n",
      "Total suggestion pairs: 470\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: Building co-purchase JSON lookup...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "TOP_PER_ITEM = 10\n",
    "co_purchase = defaultdict(list)\n",
    "\n",
    "for _, rule in rules.iterrows():\n",
    "    ant = list(rule[\"antecedents\"])[0]\n",
    "    con = list(rule[\"consequents\"])[0]\n",
    "    co_purchase[ant.lower()].append({\n",
    "        \"item\":       con,\n",
    "        \"confidence\": round(float(rule[\"confidence\"]), 3),\n",
    "        \"lift\":       round(float(rule[\"lift\"]), 2),\n",
    "        \"support\":    round(float(rule[\"support\"]), 4),\n",
    "        \"category\":   name_to_category.get(con.lower(), \"other\"),\n",
    "    })\n",
    "\n",
    "for k in co_purchase:\n",
    "    co_purchase[k] = sorted(co_purchase[k], key=lambda x: x[\"confidence\"], reverse=True)[:TOP_PER_ITEM]\n",
    "\n",
    "co_purchase_dict = dict(co_purchase)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/co_purchase_rules.json\", \"w\") as f:\n",
    "    json.dump(co_purchase_dict, f, indent=2)\n",
    "\n",
    "print(f\"Saved: {OUTPUT_DIR}/co_purchase_rules.json\")\n",
    "print(f\"Products with rules:    {len(co_purchase_dict)}\")\n",
    "print(f\"Total suggestion pairs: {sum(len(v) for v in co_purchase_dict.values())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de2b9648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 7: Spot-check — are suggestions sensible?\n",
      "============================================================\n",
      "  banana: not found\n",
      "\n",
      "  whole milk:\n",
      "    → Banana                                    conf=0.24  lift=1.5  [produce]\n",
      "\n",
      "  organic tomato basil pasta sauce:\n",
      "    → Banana                                    conf=0.23  lift=1.4  [produce]\n",
      "    → Bag of Organic Bananas                    conf=0.20  lift=1.6  [produce]\n",
      "\n",
      "  100% whole wheat bread:\n",
      "    → Banana                                    conf=0.25  lift=1.5  [produce]\n",
      "\n",
      "  air chilled organic boneless skinless chicken breasts:\n",
      "    → Bag of Organic Bananas                    conf=0.21  lift=1.6  [produce]\n",
      "\n",
      "Catalog coverage: 11.2%\n",
      "Avg rules/item:   1.4\n",
      "\n",
      "✓ NOTEBOOK 02 COMPLETE — co_purchase_rules.json ready\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 7: Spot-check — are suggestions sensible?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_items = [\"banana\", \"whole milk\", \"pasta\", \"bread\", \"chicken breast\"]\n",
    "for item in test_items:\n",
    "    key = item\n",
    "    if key not in co_purchase_dict:\n",
    "        matches = [k for k in co_purchase_dict if item in k]\n",
    "        if matches:\n",
    "            key = matches[0]\n",
    "        else:\n",
    "            print(f\"  {item}: not found\"); continue\n",
    "    print(f\"\\n  {key}:\")\n",
    "    for s in co_purchase_dict[key][:5]:\n",
    "        print(f\"    → {s['item']:40s}  conf={s['confidence']:.2f}  lift={s['lift']:.1f}  [{s['category']}]\")\n",
    "\n",
    "catalog_lower = set(i[\"name_lower\"] for i in item_catalog)\n",
    "rules_lower   = set(co_purchase_dict.keys())\n",
    "coverage = len(catalog_lower & rules_lower) / len(catalog_lower) * 100\n",
    "avg_rules = np.mean([len(v) for v in co_purchase_dict.values()])\n",
    "print(f\"\\nCatalog coverage: {coverage:.1f}%\")\n",
    "print(f\"Avg rules/item:   {avg_rules:.1f}\")\n",
    "print(\"\\n✓ NOTEBOOK 02 COMPLETE — co_purchase_rules.json ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "509802f6-cb3a-4e68-9c25-ce74dc716e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.26.4\n",
      "scipy: 1.17.1\n",
      "sklearn: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy, scipy, sklearn\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"scipy:\", scipy.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ed2bf-75f2-43a9-b768-13034e75590a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instacart-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
